{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical # 다항분포 패키지\n",
    "###\n",
    "from tqdm import tqdm # for문 돌아갈떄 확인해주는 패키지\n",
    "import numpy as np\n",
    "from preprocess import mulaw_decode # 음성파일 입력,출력 될때 8비트,16비트 맞춰줌\n",
    "                                \n",
    "# gru 모델만들기 틀만드는 과정임\n",
    "def get_gru_cell(gru):\n",
    "    gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n",
    "    gru_cell.weight_hh.data = gru.weight_hh_l0.data           # hidden hidden  # hidden과 hidden사이\n",
    "    gru_cell.weight_ih.data = gru.weight_ih_l0.data           # input hidden  # input과 hidden사이\n",
    "    gru_cell.bias_hh.data = gru.bias_hh_l0.data               \n",
    "    gru_cell.bias_ih.data = gru.bias_ih_l0.data\n",
    "    return gru_cell\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):    # 초록색 상자부분    \n",
    "    def __init__(self, in_channels, channels, n_embeddings, embedding_dim, jitter=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, channels, 3, 1, 0, bias=False),# 1차원 커널  인풋크기, 아웃풋크기, 커널크기=3, stride=1,padding=0\n",
    "            nn.BatchNorm1d(channels),                            #stride 1칸씩 이동 , 패딩=0 나머지부분 0으로 채우겠다.\n",
    "            nn.ReLU(True),             \n",
    "            nn.Conv1d(channels, channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, channels, 4, 2, 1, bias=False), # 크기 4 , 2칸씩 이동 ,1로채움\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, embedding_dim, 1)\n",
    "        )\n",
    "        self.codebook = VQEmbeddingEMA(n_embeddings, embedding_dim)  # 코드북이 벡터양자화과정에서 나오는 결과물담은벡터임 (치역)\n",
    "        self.jitter = Jitter(jitter)   # 검색해보면 jitter는 과적합방지 근데 여기선 0넣서 안한듯 decoder에서 0.5로 잡음\n",
    "\n",
    "    def forward(self, mels):\n",
    "        z = self.encoder(mels)   # 멜스펙트로그램 넣는과정\n",
    "        z, loss, perplexity = self.codebook(z.transpose(1, 2))     # 밑에부분\n",
    "        z = self.jitter(z)\n",
    "        return z, loss, perplexity\n",
    "\n",
    "    def encode(self, mel):          # 멜스펙트로그램 넣기\n",
    "        z = self.encoder(mel)\n",
    "        z, indices = self.codebook.encode(z.transpose(1, 2))\n",
    "        return z, indices\n",
    "\n",
    "# jitter가 regularization이라는데\n",
    "class Jitter(nn.Module):           # 노이즈넣어서 과적합막아주는거  # p가머지 확률같긴한데  # nn.moduler?\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        prob = torch.Tensor([p / 2, 1 - p, p / 2])   # 확률을 텐서형태로?  p=0.1 이면  0.05   0.9   0.05\n",
    "        self.register_buffer(\"prob\", prob)\n",
    "\n",
    "    def forward(self, x):                        # x가 머지\n",
    "        if not self.training or self.p == 0:   # 확률이 0이아니라면 else실행\n",
    "            return x\n",
    "        else:\n",
    "            batch_size, sample_size, channels = x.size()\n",
    "\n",
    "            dist = Categorical(self.prob)       #0.05, 0.9, 0.95 인 다항분포\n",
    "            index = dist.sample(torch.Size([batch_size, sample_size])) - 1          # ??? \n",
    "            index[:, 0].clamp_(0, 1)      # clamp 0~1사이값 반환함수\n",
    "            index[:, -1].clamp_(-1, 0)\n",
    "            index += torch.arange(sample_size, device=x.device)\n",
    "            # -1에서 1사이로 값 펴줌 논문보면 연속적으로 펴준다 그런말있는데 그건가싶기두 하고\n",
    "            x = torch.gather(x, 1, index.unsqueeze(-1).expand(-1, -1, channels))   \n",
    "        return x\n",
    "\n",
    " # 이게 벡터 양자화 과정   \n",
    "class VQEmbeddingEMA(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5):\n",
    "                        # n_embedding 임베딩벡터 개수? , embedding_dim= 임베딩 백터 크기\n",
    "        super(VQEmbeddingEMA, self).__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        init_bound = 1 / 512\n",
    "        embedding = torch.Tensor(n_embeddings, embedding_dim)   \n",
    "        embedding.uniform_(-init_bound, init_bound)      # -init~ init 균등분포 난수생성인듯\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(n_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "\n",
    "    def encode(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "          # addmm 은  n*m   m*p \n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +  \n",
    "                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        return quantized, indices\n",
    "\n",
    "    def forward(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "\n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        encodings = F.one_hot(indices, M).float()\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        #지수이동평균법 =ema\n",
    "        if self.training:\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "\n",
    "            n = torch.sum(self.ema_count)\n",
    "            self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), x_flat)\n",
    "            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
    "\n",
    "            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
    "\n",
    "        e_latent_loss = F.mse_loss(x, quantized.detach())\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, loss, perplexity\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, n_speakers, speaker_embedding_dim,\n",
    "                 conditioning_channels, mu_embedding_dim, rnn_channels,\n",
    "                 fc_channels, bits, hop_length):\n",
    "        super().__init__()\n",
    "        self.rnn_channels = rnn_channels\n",
    "        self.quantization_channels = 2**bits\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        self.speaker_embedding = nn.Embedding(n_speakers, speaker_embedding_dim)\n",
    "        self.rnn1 = nn.GRU(in_channels + speaker_embedding_dim, conditioning_channels,\n",
    "                           num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.mu_embedding = nn.Embedding(self.quantization_channels, mu_embedding_dim)\n",
    "        self.rnn2 = nn.GRU(mu_embedding_dim + 2*conditioning_channels, rnn_channels, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_channels, fc_channels)\n",
    "        self.fc2 = nn.Linear(fc_channels, self.quantization_channels)\n",
    "\n",
    "    def forward(self, x, z, speakers):\n",
    "        z = F.interpolate(z.transpose(1, 2), scale_factor=2)\n",
    "        z = z.transpose(1, 2)\n",
    "\n",
    "        speakers = self.speaker_embedding(speakers)\n",
    "        speakers = speakers.unsqueeze(1).expand(-1, z.size(1), -1)\n",
    "\n",
    "        z = torch.cat((z, speakers), dim=-1)\n",
    "        z, _ = self.rnn1(z)\n",
    "\n",
    "        z = F.interpolate(z.transpose(1, 2), scale_factor=self.hop_length)\n",
    "        z = z.transpose(1, 2)\n",
    "\n",
    "        x = self.mu_embedding(x)\n",
    "        x, _ = self.rnn2(torch.cat((x, z), dim=2))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, z, speaker):\n",
    "        output = []\n",
    "        cell = get_gru_cell(self.rnn2)\n",
    "\n",
    "        z = F.interpolate(z.transpose(1, 2), scale_factor=2)\n",
    "        z = z.transpose(1, 2)\n",
    "\n",
    "        speaker = self.speaker_embedding(speaker)\n",
    "        speaker = speaker.unsqueeze(1).expand(-1, z.size(1), -1)\n",
    "\n",
    "        z = torch.cat((z, speaker), dim=-1)\n",
    "        z, _ = self.rnn1(z)\n",
    "\n",
    "        z = F.interpolate(z.transpose(1, 2), scale_factor=self.hop_length)\n",
    "        z = z.transpose(1, 2)\n",
    "\n",
    "        batch_size, sample_size, _ = z.size()\n",
    "\n",
    "        h = torch.zeros(batch_size, self.rnn_channels, device=z.device)\n",
    "        x = torch.zeros(batch_size, device=z.device).fill_(self.quantization_channels // 2).long()\n",
    "\n",
    "        for m in tqdm(torch.unbind(z, dim=1), leave=False):\n",
    "            x = self.mu_embedding(x)\n",
    "            h = cell(torch.cat((x, m), dim=1), h)\n",
    "            x = F.relu(self.fc1(h))\n",
    "            logits = self.fc2(x)\n",
    "            dist = Categorical(logits=logits)\n",
    "            x = dist.sample()\n",
    "            output.append(2 * x.float().item() / (self.quantization_channels - 1.) - 1.)\n",
    "\n",
    "        output = np.asarray(output, dtype=np.float64)\n",
    "        output = mulaw_decode(output, self.quantization_channels)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
